---
layout : single
date: 2019-06-02
title : 线性回归和梯度下降
categories : 
  - 技术研究
tags : 
  - AI
---

### 有监督学习的两种主要任务（分类和回归）
- 有监督机器学习任务主要分为两种，一种是分类任务，一般是通过学习算法实现对未知数据的分类甄别，比如判断是男性还是女性，预测明天会不会下雨等等，预测结果是离散的分类。另一种是回归任务，用于预测一个具体的值，比如房价、年龄等等，预测结果是连续的值；
- 解决回归任务比较常见的算法就是线性回归算法；

### 线性回归
- 线性回归是最典型的用于回归任务的机器学习算法。它的基本原理是基于，任意一个为n的值，可以通过一个n-1阶的多项式进行拟合；
- 线性回归的算法的目标是根据已知标签数据，训练一个预测函数f(a, b)，使得x输入带入ax+b这样的多项式可以得出需要预测的值；
- 所以目标是找到合适的a和b（从数学语言的精确描述来说，一般用别的符号代替，为了方便记录，我使用a和b）；
- 为了找到合适的a和b，需要定义一个损失函数（也有称为代价函数的），J(a, b)，用于判断a和b是否合适，比较简单的有“方差均值”法；
- 方差均值指的是，所有样本带入预测函数f，将预测值与实际值相减，取平方，求和再求平均数（暂时不知到markdown里如何加入公式）；
- 那么带入损失函数，就是意味着，找到合适的a、b使得刚才那个方差均值小于一个阈值（接近最优）；
- 线性回归本质上，是在平面坐标系里找到一个与样本匹配的直线，这样基于这个直线，即可预测未知的值；

### 梯度下降
- 如何找到合适的a和b，一个一个地去试是不可能的（无穷多的计算量）；
- 比较典型的算法是“梯度下降”，又是牛顿起的头；
- 比较生动的描述是，想想一个三维空间图形，X和Y平面是有a和b的值构成，Z轴是损失函数的值，值最小的部分（在图上就最低的部分，像一个山谷）就是最优解；
- 从山顶的某个点上，环顾360度，找到适合下山的一小步，一点一点儿走向山谷最低处，有点像慢慢走楼梯，所以叫梯度下降；
- 从我的体会来看，这个方法有点像牛顿那个计算平方根的迭代法（根据一个猜测值，然后通过迭代公式寻找一个更加接近正确值的值，直到误差降低到合适范围）；
- 从微积分的角度，就是找到损失函数J在最初尝试的那个点上的切线，并通过它的斜率（也就是导数值）乘以一个系数a，逐步逼近最低点（切线斜率为零）的过程；